# ROC AUC
##  AUC ROC

При обращении вероятности в бинарную метку (относится к классу или нет), мы должны выбрать какой-то порог.

ROC AUC кривая показывает, как будет отрабатывать модель при изменении этого порога от 0 к 1.

По сути, это вероятность того, что случайный "+" объект получит оценку выше случайного '-' объекта, Что прямо следует из определения [[вероятность|вероятности]]

Это линия от (0,0) до (1,1) в координатах True Positive Rate (TPR) и False Positive Rate (FPR)

Доля верных положительных классификаций
$$TPR = \frac{TP}{TP+FN}$$
TPR это [[recall]]

Доля ложных положительных классификаций
$$FPR = \frac{FP}{FP+TN}$$

FPR это [[recall]] "наоборот" . Показывает долю "негативных" объектов предсказанных неверно.

Плавно изменяя "порог" от 0 к 1 и отмечая по осям TPR и FPR, получим ROC-кривую. (на факте она ломаная из вертикальных и горизонтальных отрезках, но при большом объеме выборки сглаживается). 
В идеале, кривая стремится к точке TPR = 1 и FPR = 0
Случайный классификатор будет иметь линейную зависимость.
AUC - площадь под кривой, для него будет 0.5 (для рабочего классификатора 0.5 и меньше - неадекватно). AUC => 1 - хороший результат.

ROC AUC позволяет оценить оптимальную "отсечку".

```python
import numpy as np
from sklearn.metrics  import roc_curve

sns.set(font_scale=1.5)
sns.set_color_codes("muted")

plt.figure(figsize=(10, 8))
fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1], pos_label=1)
lw = 2
plt.plot(fpr, tpr, lw=lw, label='ROC curve ')
plt.plot([0, 1], [0, 1])
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve')
plt.savefig("ROC.png")
plt.show()
```
